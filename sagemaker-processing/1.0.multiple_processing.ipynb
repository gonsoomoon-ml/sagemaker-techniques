{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Preprocessing through Two Instances on SageMaker\n",
    "* __Techniques included:__\n",
    "    * Test a preprocessing script on notebook\n",
    "    * Prepreocess the preprocessing script on two instances\n",
    "\n",
    "\n",
    "* Reference:\n",
    "    - Code Reference: https://github.com/aws-samples/amazon-sagemaker-script-mode/tree/master/Mtf-2-workflow\n",
    "    - Related Blog: https://aws.amazon.com/ko/blogs/machine-learning/creating-a-complete-tensorflow-2-workflow-in-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Processing numerical data on Boston Pricing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders \n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "# import tensorflow as tf\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() \n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-057716757052/preprocess-two-instances/data/raw\n"
     ]
    }
   ],
   "source": [
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "s3_prefix = 'preprocess-two-instances'\n",
    "rawdata_s3_prefix = '{}/data/raw'.format(s3_prefix)\n",
    "raw_s3 = sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
       "        3.96900e+02, 1.87200e+01],\n",
       "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
       "        3.95380e+02, 3.11000e+00],\n",
       "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.75520e+02, 3.26000e+00],\n",
       "       ...,\n",
       "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
       "        3.62250e+02, 7.83000e+00],\n",
       "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
       "        2.61950e+02, 1.57900e+01],\n",
       "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
       "        3.76700e+02, 4.38000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"data/raw/x_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_data_path = \"data/processing/train\"\n",
    "output_test_data_path = \"data/processing/test\"\n",
    "! mkdir -p {output_train_data_path}\n",
    "! mkdir -p {output_test_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gs-preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gs-preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Set input_data_path to internal folder on docker\n",
    "    input_data_path = '/opt/ml/processing/input'\n",
    "    parser.add_argument('--input_data_path', type=str, default=input_data_path)\n",
    "    \n",
    "    # output_train_data_path\n",
    "    output_train_data_path = '/opt/ml/processing/train'\n",
    "    parser.add_argument('--output_train_data_path', type=str, default=output_train_data_path)    \n",
    "    # output_test_data_path\n",
    "    output_test_data_path = '/opt/ml/processing/test'\n",
    "    parser.add_argument('--output_test_data_path', type=str, default=output_test_data_path)    \n",
    "\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    \n",
    "    input_path = '{}/*.npy'.format(args.input_data_path)\n",
    "    print(\"input_path:\\n \", input_path)\n",
    "    input_files = glob.glob(input_path)    \n",
    "    print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    for file in input_files:\n",
    "        raw = np.load(file)\n",
    "        transformed = scaler.fit_transform(raw)\n",
    "        print(\"transformed: \\n\", transformed[0])\n",
    "        if 'train' in file:\n",
    "#             output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "            output_path = os.path.join(args.output_train_data_path, 'x_train.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "#             output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "            output_path = os.path.join(args.output_test_data_path, 'x_test.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TEST DATA FILE\\n')            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/data/raw'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Script Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received arguments Namespace(input_data_path='/home/ec2-user/SageMaker/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/data/raw', output_test_data_path='data/processing/test', output_train_data_path='data/processing/train')\n",
      "input_path:\n",
      "  /home/ec2-user/SageMaker/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/data/raw/*.npy\n",
      "\n",
      "INPUT FILE LIST: \n",
      "['/home/ec2-user/SageMaker/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/data/raw/x_train.npy', '/home/ec2-user/SageMaker/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/data/raw/x_test.npy']\n",
      "\n",
      "transformed: \n",
      " [-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\n",
      "SAVED TRANSFORMED TRAINING DATA FILE\n",
      "\n",
      "transformed: \n",
      " [ 2.8040301  -0.50784934  0.96960877 -0.32969024  1.23174581  0.11934137\n",
      "  1.14739788 -0.91935276  1.60609286  1.40778227  0.90513041 -4.27829517\n",
      "  2.51324773]\n",
      "SAVED TRANSFORMED TEST DATA FILE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python gs-preprocessing.py --input_data_path {raw_dir} --output_train_data_path {output_train_data_path} --output_test_data_path {output_test_data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Two Instances to transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=get_execution_role(),\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  Preprocess-two-instances-02-14-24-26\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/preprocess-two-instances/data/raw', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/Preprocess-two-instances-02-14-24-26/input/code/gs-preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/preprocess-two-instances/data/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/preprocess-two-instances/data/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........................\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(input_data_path='/opt/ml/processing/input', output_test_data_path='/opt/ml/processing/test', output_train_data_path='/opt/ml/processing/train')\u001b[0m\n",
      "\u001b[34minput_path:\n",
      "  /opt/ml/processing/input/*.npy\n",
      "\u001b[0m\n",
      "\u001b[34mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/input/x_test.npy']\n",
      "\u001b[0m\n",
      "\u001b[34mtransformed: \n",
      " [ 2.8040301  -0.50784934  0.96960877 -0.32969024  1.23174581  0.11934137\n",
      "  1.14739788 -0.91935276  1.60609286  1.40778227  0.90513041 -4.27829517\n",
      "  2.51324773]\u001b[0m\n",
      "\u001b[34mSAVED TRANSFORMED TEST DATA FILE\n",
      "\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35mReceived arguments Namespace(input_data_path='/opt/ml/processing/input', output_test_data_path='/opt/ml/processing/test', output_train_data_path='/opt/ml/processing/train')\u001b[0m\n",
      "\u001b[35minput_path:\n",
      "  /opt/ml/processing/input/*.npy\n",
      "\u001b[0m\n",
      "\u001b[35mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[35m['/opt/ml/processing/input/x_train.npy']\n",
      "\u001b[0m\n",
      "\u001b[35mtransformed: \n",
      " [-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\u001b[0m\n",
      "\u001b[35mSAVED TRANSFORMED TRAINING DATA FILE\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime\n",
    "\n",
    "processing_job_name = \"Preprocess-two-instances-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "\n",
    "sklearn_processor.run(code = 'gs-preprocessing.py',\n",
    "                      job_name = processing_job_name,\n",
    "                      inputs = [ProcessingInput(\n",
    "                          source = raw_s3,\n",
    "                          destination = '/opt/ml/processing/input',\n",
    "                          s3_data_distribution_type = 'ShardedByS3Key')],\n",
    "                      outputs = [ProcessingOutput(output_name = 'train',\n",
    "                                                 destination = '{}/train'.format(output_destination),\n",
    "                                                 source = '/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test',\n",
    "                                                destination='{}/test'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/test')])\n",
    "\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Example to Read two csv files on each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_train_folder = \"processing/train\"\n",
    "process_test_folder = \"processing/test\"\n",
    "! mkdir -p {process_train_folder}\n",
    "! mkdir -p {process_test_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199523, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>class of worker</th>\n",
       "      <th>detailed industry recode</th>\n",
       "      <th>detailed occupation recode</th>\n",
       "      <th>education</th>\n",
       "      <th>wage per hour</th>\n",
       "      <th>enroll in edu inst last wk</th>\n",
       "      <th>marital stat</th>\n",
       "      <th>major industry code</th>\n",
       "      <th>major occupation code</th>\n",
       "      <th>...</th>\n",
       "      <th>country of birth father</th>\n",
       "      <th>country of birth mother</th>\n",
       "      <th>country of birth self</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>own business or self employed</th>\n",
       "      <th>fill inc questionnaire for veteran's admin</th>\n",
       "      <th>veterans benefits</th>\n",
       "      <th>weeks worked in year</th>\n",
       "      <th>year</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High school graduate</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Self-employed-not incorporated</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Precision production craft &amp; repair</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10th grade</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Foreign born- Not a citizen of U S</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                  class of worker  detailed industry recode  \\\n",
       "0   73                  Not in universe                         0   \n",
       "1   58   Self-employed-not incorporated                         4   \n",
       "2   18                  Not in universe                         0   \n",
       "\n",
       "   detailed occupation recode                    education  wage per hour  \\\n",
       "0                           0         High school graduate              0   \n",
       "1                          34   Some college but no degree              0   \n",
       "2                           0                   10th grade              0   \n",
       "\n",
       "  enroll in edu inst last wk    marital stat           major industry code  \\\n",
       "0            Not in universe         Widowed   Not in universe or children   \n",
       "1            Not in universe        Divorced                  Construction   \n",
       "2                High school   Never married   Not in universe or children   \n",
       "\n",
       "                  major occupation code  ... country of birth father  \\\n",
       "0                       Not in universe  ...           United-States   \n",
       "1   Precision production craft & repair  ...           United-States   \n",
       "2                       Not in universe  ...                 Vietnam   \n",
       "\n",
       "  country of birth mother country of birth self  \\\n",
       "0           United-States         United-States   \n",
       "1           United-States         United-States   \n",
       "2                 Vietnam               Vietnam   \n",
       "\n",
       "                            citizenship own business or self employed  \\\n",
       "0     Native- Born in the United States                             0   \n",
       "1     Native- Born in the United States                             0   \n",
       "2   Foreign born- Not a citizen of U S                              0   \n",
       "\n",
       "  fill inc questionnaire for veteran's admin  veterans benefits  \\\n",
       "0                            Not in universe                  2   \n",
       "1                            Not in universe                  2   \n",
       "2                            Not in universe                  2   \n",
       "\n",
       "   weeks worked in year  year     income  \n",
       "0                     0    95   - 50000.  \n",
       "1                    52    94   - 50000.  \n",
       "2                     0    95   - 50000.  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "region = 'us-east-1'\n",
    "input_data = 's3://sagemaker-sample-data-{}/processing/census/census-income.csv'.format(region)\n",
    "\n",
    "s3_input_data = 's3://sagemaker-sample-data-{}/processing/census'.format(region)\n",
    "# df = pd.read_csv(input_data, nrows=10)\n",
    "df = pd.read_csv(input_data)\n",
    "print(df.shape)\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write script to Read each csv file on each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gs-csv-preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gs-csv-preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Set input_data_path to internal folder on docker\n",
    "    input_data_path = '/opt/ml/processing/input'\n",
    "    parser.add_argument('--input_data_path', type=str, default=input_data_path)\n",
    "    \n",
    "    # output_train_data_path\n",
    "    output_train_data_path = '/opt/ml/processing/train'\n",
    "    parser.add_argument('--output_train_data_path', type=str, default=output_train_data_path)    \n",
    "    # output_test_data_path\n",
    "    output_test_data_path = '/opt/ml/processing/test'\n",
    "    parser.add_argument('--output_test_data_path', type=str, default=output_test_data_path)    \n",
    "\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    input_path = '{}/*.csv'.format(args.input_data_path)\n",
    "    print(\"input_path:\\n \", input_path)\n",
    "    input_files = glob.glob(input_path)    \n",
    "    print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download two csv files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/csv’: File exists\n"
     ]
    }
   ],
   "source": [
    "csv_folder = 'data/csv'\n",
    "! mkdir {csv_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv to data/csv/census-income-01.csv\n"
     ]
    }
   ],
   "source": [
    "census_file_path = 's3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv'\n",
    "!aws s3 cp {census_file_path} data/csv/census-income-01.csv\n",
    "! cp census-income-01.csv data/csv/census-income-02.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload two csv files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'processing/csv-example'\n",
    "s3_input_path = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "s3_output_path = 's3://{}/{}/data'.format(bucket, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/csv/census-income-01.csv to s3://sagemaker-us-east-1-057716757052/processing/csv-example/data/census-income-01.csv\n",
      "upload: data/csv/census-income-02.csv to s3://sagemaker-us-east-1-057716757052/processing/csv-example/data/census-income-02.csv\n",
      "2020-06-02 14:29:17  103875379 census-income-01.csv\n",
      "2020-06-02 14:29:18  103875379 census-income-02.csv\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp data/csv/census-income-01.csv {s3_input_path}/\n",
    "! aws s3 cp data/csv/census-income-02.csv {s3_input_path}/\n",
    "! aws s3 ls {s3_input_path}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received arguments Namespace(input_data_path='data/csv', output_test_data_path='/opt/ml/processing/test', output_train_data_path='/opt/ml/processing/train')\n",
      "input_path:\n",
      "  data/csv/*.csv\n",
      "\n",
      "INPUT FILE LIST: \n",
      "['data/csv/census-income-02.csv', 'data/csv/census-income-01.csv']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python gs-csv-preprocessing.py  --input_data_path {csv_folder} --process_train_folder {process_train_folder} --process_test_folder {process_train_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=get_execution_role(),\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  processing-workflow-02-14-29-20\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/csv-example/data', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing-workflow-02-14-29-20/input/code/gs-csv-preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/output/data/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/output/data/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(input_data_path='/opt/ml/processing/input', output_test_data_path='/opt/ml/processing/test', output_train_data_path='/opt/ml/processing/train')\u001b[0m\n",
      "\u001b[34minput_path:\n",
      "  /opt/ml/processing/input/*.csv\n",
      "\u001b[0m\n",
      "\u001b[34mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/input/census-income-01.csv']\n",
      "\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[35mReceived arguments Namespace(input_data_path='/opt/ml/processing/input', output_test_data_path='/opt/ml/processing/test', output_train_data_path='/opt/ml/processing/train')\u001b[0m\n",
      "\u001b[35minput_path:\n",
      "  /opt/ml/processing/input/*.csv\n",
      "\u001b[0m\n",
      "\u001b[35mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[35m['/opt/ml/processing/input/census-income-02.csv']\n",
      "\u001b[0m\n",
      "\n",
      "CPU times: user 586 ms, sys: 29.3 ms, total: 615 ms\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime\n",
    "\n",
    "processing_job_name = \"processing-workflow-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "\n",
    "sklearn_processor.run(code = 'gs-csv-preprocessing.py',\n",
    "                      job_name = processing_job_name,\n",
    "                      inputs = [ProcessingInput(\n",
    "                          source = s3_input_path,                          \n",
    "                          destination = '/opt/ml/processing/input',\n",
    "                          s3_data_distribution_type = 'ShardedByS3Key')],\n",
    "                      outputs = [ProcessingOutput(output_name = 'train',\n",
    "                                                 destination = '{}/train'.format(s3_output_path),\n",
    "                                                 source = '/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test',\n",
    "                                                destination='{}/test'.format(s3_output_path),\n",
    "                                                source='/opt/ml/processing/test')])\n",
    "\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
