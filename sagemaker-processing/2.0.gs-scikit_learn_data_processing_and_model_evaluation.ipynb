{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Processing jobs\n",
    "- __Modified Points__:\n",
    "    - Make new preprocess script for a local test and use it on the following code\n",
    "    \n",
    "    \n",
    "- Original: https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Processing jobs\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform.\n",
    "\n",
    "A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "<img src=\"Processing-1.jpg\">\n",
    "\n",
    "This notebook shows how you can:\n",
    "\n",
    "1. Run a processing job to run a scikit-learn script that cleans, pre-processes, performs feature engineering, and splits the input data into train and test sets.\n",
    "2. Run a training job on the pre-processed training data to train a model\n",
    "3. Run a processing job on the pre-processed test data to evaluate the trained model's performance\n",
    "4. Use your own custom container to run processing jobs with your own Python libraries and dependencies.\n",
    "\n",
    "The dataset used here is the [Census-Income KDD Dataset](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29). You select features from this dataset, clean the data, and turn the data into features that the training algorithm can use to train a binary classification model, and split the data into train and test sets. The task is to predict whether rows representing census responders have an income greater than `$50,000`, or less than `$50,000`. The dataset is heavily class imbalanced, with most records being labeled as earning less than `$50,000`. After training a logistic regression model, you evaluate the model against a hold-out test dataset, and save the classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the scikit-learn preprocessing script as a processing job, create a `SKLearnProcessor`, which lets you run scripts inside of processing jobs using the scikit-learn image provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before introducing the script you use for data cleaning, pre-processing, and feature engineering, inspect the first 20 rows of the dataset. The target is predicting the `income` category. The features from the dataset you select are `age`, `education`, `major industry code`, `class of worker`, `num persons worked for employer`, `capital gains`, `capital losses`, and `dividends from stocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199523, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>class of worker</th>\n",
       "      <th>detailed industry recode</th>\n",
       "      <th>detailed occupation recode</th>\n",
       "      <th>education</th>\n",
       "      <th>wage per hour</th>\n",
       "      <th>enroll in edu inst last wk</th>\n",
       "      <th>marital stat</th>\n",
       "      <th>major industry code</th>\n",
       "      <th>major occupation code</th>\n",
       "      <th>...</th>\n",
       "      <th>country of birth father</th>\n",
       "      <th>country of birth mother</th>\n",
       "      <th>country of birth self</th>\n",
       "      <th>citizenship</th>\n",
       "      <th>own business or self employed</th>\n",
       "      <th>fill inc questionnaire for veteran's admin</th>\n",
       "      <th>veterans benefits</th>\n",
       "      <th>weeks worked in year</th>\n",
       "      <th>year</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High school graduate</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Self-employed-not incorporated</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>Some college but no degree</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Precision production craft &amp; repair</td>\n",
       "      <td>...</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>94</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10th grade</td>\n",
       "      <td>0</td>\n",
       "      <td>High school</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Not in universe or children</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>...</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Foreign born- Not a citizen of U S</td>\n",
       "      <td>0</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>- 50000.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                  class of worker  detailed industry recode  \\\n",
       "0   73                  Not in universe                         0   \n",
       "1   58   Self-employed-not incorporated                         4   \n",
       "2   18                  Not in universe                         0   \n",
       "\n",
       "   detailed occupation recode                    education  wage per hour  \\\n",
       "0                           0         High school graduate              0   \n",
       "1                          34   Some college but no degree              0   \n",
       "2                           0                   10th grade              0   \n",
       "\n",
       "  enroll in edu inst last wk    marital stat           major industry code  \\\n",
       "0            Not in universe         Widowed   Not in universe or children   \n",
       "1            Not in universe        Divorced                  Construction   \n",
       "2                High school   Never married   Not in universe or children   \n",
       "\n",
       "                  major occupation code  ... country of birth father  \\\n",
       "0                       Not in universe  ...           United-States   \n",
       "1   Precision production craft & repair  ...           United-States   \n",
       "2                       Not in universe  ...                 Vietnam   \n",
       "\n",
       "  country of birth mother country of birth self  \\\n",
       "0           United-States         United-States   \n",
       "1           United-States         United-States   \n",
       "2                 Vietnam               Vietnam   \n",
       "\n",
       "                            citizenship own business or self employed  \\\n",
       "0     Native- Born in the United States                             0   \n",
       "1     Native- Born in the United States                             0   \n",
       "2   Foreign born- Not a citizen of U S                              0   \n",
       "\n",
       "  fill inc questionnaire for veteran's admin  veterans benefits  \\\n",
       "0                            Not in universe                  2   \n",
       "1                            Not in universe                  2   \n",
       "2                            Not in universe                  2   \n",
       "\n",
       "   weeks worked in year  year     income  \n",
       "0                     0    95   - 50000.  \n",
       "1                    52    94   - 50000.  \n",
       "2                     0    95   - 50000.  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_data = 's3://sagemaker-sample-data-{}/processing/census/census-income.csv'.format(region)\n",
    "s3_input_data = 's3://sagemaker-sample-data-{}/processing/census'.format(region)\n",
    "# df = pd.read_csv(input_data, nrows=10)\n",
    "df = pd.read_csv(input_data)\n",
    "print(df.shape)\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_input_path:  s3://sagemaker-us-east-1-057716757052/processing/input\n",
      "s3_output_path:  s3://sagemaker-us-east-1-057716757052/processing/output\n"
     ]
    }
   ],
   "source": [
    "bucket = 'sagemaker-us-east-1-057716757052'\n",
    "s3_prefix_input = \"processing/input\"\n",
    "s3_prefix_output = \"processing/output\"\n",
    "s3_input_path = 's3://{}/{}'.format(bucket, s3_prefix_input)\n",
    "s3_output_path = 's3://{}/{}'.format(bucket, s3_prefix_output)\n",
    "print(\"s3_input_path: \", s3_input_path)\n",
    "print(\"s3_output_path: \", s3_output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun this cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. In this script, you\n",
    "\n",
    "* Remove duplicates and rows with conflicting data\n",
    "* transform the target `income` column into a column containing two labels.\n",
    "* transform the `age` and `num persons worked for employer` numerical columns into categorical features by binning them\n",
    "* scale the continuous `capital gains`, `capital losses`, and `dividends from stocks` so they're suitable for training\n",
    "* encode the `education`, `major industry code`, `class of worker` so they're suitable for training\n",
    "* split the data into training and test datasets, and saves the training features and labels and test features and labels.\n",
    "\n",
    "Our training script will use the pre-processed training features and labels to train a model, and our model evaluation script will use the trained model and pre-processed test features and labels to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make processing folder as a local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_train_folder = \"processing/train\"\n",
    "process_test_folder = \"processing/test\"\n",
    "! mkdir -p {process_train_folder}\n",
    "! mkdir -p {process_test_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a processing script for a local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gs-preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gs-preprocessing.py\n",
    "    \n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action = 'ignore', category=DataConversionWarning)\n",
    "\n",
    "columns = ['age', 'education', 'major industry code', 'class of worker', 'num persons worked for employer',\n",
    "           'capital gains', 'capital losses', 'dividends from stocks', 'income']\n",
    "class_labels = [' - 50000.', ' 50000+.']\n",
    "\n",
    "def print_shape(df):\n",
    "    negative_examples, positive_examples = np.bincount(df['income'])\n",
    "    print('Data shape: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))    \n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "\n",
    "    # Set fronm Docker without command line argument input_data_path\n",
    "    dockert_basic_file_path = '/opt/ml/processing/input/census-income.csv' \n",
    "    parser.add_argument('--input_data_path', type=str, default=dockert_basic_file_path)\n",
    "    \n",
    "    # output_train_data_path\n",
    "    output_train_data_path = '/opt/ml/processing/train' \n",
    "    parser.add_argument('--process_train_folder', type=str, default=output_train_data_path)    \n",
    "    # output_test_data_path\n",
    "    output_test_data_path = '/opt/ml/processing/test'\n",
    "    parser.add_argument('--process_test_folder', type=str, default=output_test_data_path)    \n",
    "\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    # Set from Docker\n",
    "    input_data_path = args.input_data_path\n",
    "#     input_data_path = os.path.join(input_data_path, 'census-income.csv')\n",
    "\n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "\n",
    "    ###########################################\n",
    "    # Drop na, remove duplicates\n",
    "\n",
    "    df = pd.read_csv(input_data_path, engine='python')\n",
    "    df = pd.DataFrame(data = df, columns = columns)\n",
    "    df.dropna(inplace = True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.replace(class_labels, [0,1], inplace=True)\n",
    "\n",
    "    print_shape(df)\n",
    "    print(df.head(2))\n",
    "\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    print('Splitting data into train and test sets with ratio {}'.format(split_ratio))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('income', axis=1), df['income'], test_size = split_ratio, random_state=0)\n",
    "\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        (['age', 'num persons worked for employer'], KBinsDiscretizer(encode='onehot-dense', n_bins=10)),\n",
    "        (['capital gains', 'capital losses', 'dividends from stocks'], StandardScaler()),\n",
    "        (['education', 'major industry code', 'class of worker'], OneHotEncoder(sparse=False))\n",
    "    )\n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = preprocess.fit_transform(X_train)\n",
    "    test_features = preprocess.transform(X_test)\n",
    "\n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    print(\"train transformed features: \\n\", train_features[0])\n",
    "    \n",
    "    train_features_output_path = os.path.join(args.process_train_folder, 'train_features.csv')\n",
    "    train_labels_output_path = os.path.join(args.process_train_folder, 'train_labels.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join(args.process_test_folder, 'test_features.csv')\n",
    "    test_labels_output_path = os.path.join(args.process_test_folder, 'test_labels.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "    \n",
    "    print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received arguments Namespace(input_data_path='s3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv', process_test_folder='processing/train', process_train_folder='processing/train', train_test_split_ratio=0.3)\n",
      "Reading input data from s3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv\n",
      "Data shape: (68285, 9), 11401 positive examples, 56884 negative examples\n",
      "   age                    education  ... dividends from stocks income\n",
      "0   73         High school graduate  ...                     0      0\n",
      "1   58   Some college but no degree  ...                     0      0\n",
      "\n",
      "[2 rows x 9 columns]\n",
      "Splitting data into train and test sets with ratio 0.3\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py:732: DeprecationWarning: `make_column_transformer` now expects (transformer, columns) as input tuples instead of (columns, transformer). This has been introduced in v0.20.1. `make_column_transformer` will stop accepting the deprecated (columns, transformer) order in v0.22.\n",
      "  warnings.warn(message, DeprecationWarning)\n",
      "Running preprocessing and feature engineering transformations\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "Train data shape after preprocessing: (47799, 69)\n",
      "Test data shape after preprocessing: (20486, 69)\n",
      "train transformed features: \n",
      " [ 0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.         -0.15860928 -0.23754907\n",
      " -0.16885653  0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.        ]\n",
      "Saving training features to processing/train/train_features.csv\n",
      "Saving test features to processing/train/test_features.csv\n",
      "Saving training labels to processing/train/train_labels.csv\n",
      "Saving test labels to processing/train/test_labels.csv\n"
     ]
    }
   ],
   "source": [
    "! python gs-preprocessing.py  --input_data_path {input_data} --process_train_folder {process_train_folder} --process_test_folder {process_train_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile preprocessing.py\n",
    "\n",
    "# import argparse\n",
    "# import os\n",
    "# import warnings\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.compose import make_column_transformer\n",
    "\n",
    "# from sklearn.exceptions import DataConversionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# columns = ['age', 'education', 'major industry code', 'class of worker', 'num persons worked for employer',\n",
    "#            'capital gains', 'capital losses', 'dividends from stocks', 'income']\n",
    "# class_labels = [' - 50000.', ' 50000+.']\n",
    "\n",
    "# def print_shape(df):\n",
    "#     negative_examples, positive_examples = np.bincount(df['income'])\n",
    "#     print('Data shape: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "#     args, _ = parser.parse_known_args()\n",
    "    \n",
    "#     print('Received arguments {}'.format(args))\n",
    "\n",
    "#     input_data_path = os.path.join('/opt/ml/processing/input', 'census-income.csv')\n",
    "    \n",
    "#     print('Reading input data from {}'.format(input_data_path))\n",
    "#     df = pd.read_csv(input_data_path)\n",
    "#     df = pd.DataFrame(data=df, columns=columns)\n",
    "#     df.dropna(inplace=True)\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     df.replace(class_labels, [0, 1], inplace=True)\n",
    "    \n",
    "#     negative_examples, positive_examples = np.bincount(df['income'])\n",
    "#     print('Data after cleaning: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))\n",
    "    \n",
    "#     split_ratio = args.train_test_split_ratio\n",
    "#     print('Splitting data into train and test sets with ratio {}'.format(split_ratio))\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(df.drop('income', axis=1), df['income'], test_size=split_ratio, random_state=0)\n",
    "\n",
    "#     preprocess = make_column_transformer(\n",
    "#         (['age', 'num persons worked for employer'], KBinsDiscretizer(encode='onehot-dense', n_bins=10)),\n",
    "#         (['capital gains', 'capital losses', 'dividends from stocks'], StandardScaler()),\n",
    "#         (['education', 'major industry code', 'class of worker'], OneHotEncoder(sparse=False))\n",
    "#     )\n",
    "#     print('Running preprocessing and feature engineering transformations')\n",
    "#     train_features = preprocess.fit_transform(X_train)\n",
    "#     test_features = preprocess.transform(X_test)\n",
    "    \n",
    "#     print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "#     print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "#     train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv')\n",
    "#     train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv')\n",
    "    \n",
    "#     test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "#     test_labels_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "    \n",
    "#     print('Saving training features to {}'.format(train_features_output_path))\n",
    "#     pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "#     print('Saving test features to {}'.format(test_features_output_path))\n",
    "#     pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)\n",
    "    \n",
    "#     print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "#     y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "    \n",
    "#     print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "#     y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job. Use the `SKLearnProcessor.run()` method. You give the `run()` method one `ProcessingInput` where the `source` is the census dataset in Amazon S3, and the `destination` is where the script reads this data from, in this case `/opt/ml/processing/input`. These local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to. For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name/`. You also give the ProcessingOutputs values for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The `arguments` parameter in the `run()` method are command-line arguments in our `preprocessing.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  process-workflow-02-14-26-41\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/process-workflow-02-14-26-41/input/code/gs-preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(input_data_path='/opt/ml/processing/input/census-income.csv', process_test_folder='/opt/ml/processing/test', process_train_folder='/opt/ml/processing/train', train_test_split_ratio=0.3)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/census-income.csv\u001b[0m\n",
      "\u001b[34mData shape: (68285, 9), 11401 positive examples, 56884 negative examples\n",
      "   age                    education  ... dividends from stocks income\u001b[0m\n",
      "\u001b[34m0   73         High school graduate  ...                     0      0\u001b[0m\n",
      "\u001b[34m1   58   Some college but no degree  ...                     0      0\n",
      "\u001b[0m\n",
      "\u001b[34m[2 rows x 9 columns]\u001b[0m\n",
      "\u001b[34mSplitting data into train and test sets with ratio 0.3\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (47799, 73)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (20486, 73)\u001b[0m\n",
      "\u001b[34mtrain transformed features: \n",
      " [ 0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.         -0.15860928 -0.23754907 -0.16885653  0.\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.        ]\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/test/test_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving test labels to /opt/ml/processing/test/test_labels.csv\u001b[0m\n",
      "CPU times: user 536 ms, sys: 50.1 ms, total: 586 ms\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=get_execution_role(),\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime\n",
    "\n",
    "processing_job_name = \"process-workflow-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "\n",
    "\n",
    "sklearn_processor.run(code = 'gs-preprocessing.py',\n",
    "                      job_name = processing_job_name,\n",
    "                      inputs = [ProcessingInput(\n",
    "                          source = input_data,\n",
    "                          destination = '/opt/ml/processing/input',\n",
    "                          s3_data_distribution_type = 'ShardedByS3Key')],\n",
    "                      outputs = [ProcessingOutput(output_name = 'train',\n",
    "                                                 destination = '{}/train'.format(s3_output_path),\n",
    "                                                 source = '/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test',\n",
    "                                                destination='{}/test'.format(s3_output_path),\n",
    "                                                source='/opt/ml/processing/test')])\n",
    "\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect the output of the pre-processing job, which consists of the processed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'input-1',\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv',\n",
       "    'LocalPath': '/opt/ml/processing/input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'ShardedByS3Key',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'code',\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/process-workflow-02-14-26-41/input/code/gs-preprocessing.py',\n",
       "    'LocalPath': '/opt/ml/processing/input/code',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/train',\n",
       "     'LocalPath': '/opt/ml/processing/train',\n",
       "     'S3UploadMode': 'EndOfJob'}},\n",
       "   {'OutputName': 'test',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/test',\n",
       "     'LocalPath': '/opt/ml/processing/test',\n",
       "     'S3UploadMode': 'EndOfJob'}}]},\n",
       " 'ProcessingJobName': 'process-workflow-02-14-26-41',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3',\n",
       "  'ContainerEntrypoint': ['python3',\n",
       "   '/opt/ml/processing/input/code/gs-preprocessing.py']},\n",
       " 'RoleArn': 'arn:aws:iam::057716757052:role/service-role/AmazonSageMaker-ExecutionRole-20191128T110038',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:057716757052:processing-job/process-workflow-02-14-26-41',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2020, 6, 2, 14, 30, 41, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2020, 6, 2, 14, 30, 19, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 6, 2, 14, 30, 41, 721000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2020, 6, 2, 14, 26, 41, 885000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'ca4b6023-4ceb-44b6-a73b-418e99acb007',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ca4b6023-4ceb-44b6-a73b-418e99acb007',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1770',\n",
       "   'date': 'Tue, 02 Jun 2020 14:30:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outputs': [{'OutputName': 'train',\n",
       "   'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/train',\n",
       "    'LocalPath': '/opt/ml/processing/train',\n",
       "    'S3UploadMode': 'EndOfJob'}},\n",
       "  {'OutputName': 'test',\n",
       "   'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/test',\n",
       "    'LocalPath': '/opt/ml/processing/test',\n",
       "    'S3UploadMode': 'EndOfJob'}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "output_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-057716757052/processing/output/train\n",
      "2020-06-02 14:30:36   16369870 train_features.csv\n",
      "2020-06-02 14:30:36      95598 train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_training_data)\n",
    "! aws s3 ls {preprocessed_training_data}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (47798, 73)\n"
     ]
    }
   ],
   "source": [
    "training_features = pd.read_csv(preprocessed_training_data + '/train_features.csv')\n",
    "print('Training features shape: {}'.format(training_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (10, 73)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>0.0.2</th>\n",
       "      <th>0.0.3</th>\n",
       "      <th>0.0.4</th>\n",
       "      <th>0.0.5</th>\n",
       "      <th>0.0.6</th>\n",
       "      <th>0.0.7</th>\n",
       "      <th>0.0.8</th>\n",
       "      <th>...</th>\n",
       "      <th>0.0.56</th>\n",
       "      <th>0.0.57</th>\n",
       "      <th>0.0.58</th>\n",
       "      <th>0.0.59</th>\n",
       "      <th>0.0.60</th>\n",
       "      <th>1.0.4</th>\n",
       "      <th>0.0.61</th>\n",
       "      <th>0.0.62</th>\n",
       "      <th>0.0.63</th>\n",
       "      <th>0.0.64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.0  1.0  0.0.1  0.0.2  0.0.3  0.0.4  0.0.5  0.0.6  0.0.7  0.0.8  ...  \\\n",
       "0  0.0  0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0  ...   \n",
       "2  0.0  0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3  0.0  1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4  0.0  0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "5  1.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "6  0.0  0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "7  0.0  0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "8  0.0  0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0  ...   \n",
       "9  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  ...   \n",
       "\n",
       "   0.0.56  0.0.57  0.0.58  0.0.59  0.0.60  1.0.4  0.0.61  0.0.62  0.0.63  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0    1.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0    1.0     0.0     0.0     0.0   \n",
       "2     0.0     1.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0    1.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     1.0    0.0     0.0     0.0     0.0   \n",
       "5     0.0     0.0     0.0     0.0     0.0    1.0     0.0     0.0     0.0   \n",
       "6     0.0     0.0     0.0     0.0     0.0    0.0     0.0     1.0     0.0   \n",
       "7     0.0     0.0     0.0     0.0     0.0    1.0     0.0     0.0     0.0   \n",
       "8     0.0     0.0     0.0     0.0     0.0    1.0     0.0     0.0     0.0   \n",
       "9     0.0     0.0     0.0     0.0     1.0    0.0     0.0     0.0     0.0   \n",
       "\n",
       "   0.0.64  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "5     0.0  \n",
       "6     0.0  \n",
       "7     0.0  \n",
       "8     0.0  \n",
       "9     0.0  \n",
       "\n",
       "[10 rows x 73 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features = pd.read_csv(preprocessed_training_data + '/train_features.csv', nrows=10)\n",
    "print('Training features shape: {}'.format(training_features.shape))\n",
    "training_features.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using the pre-processed data\n",
    "\n",
    "We create a `SKLearn` instance, which we will use to run a training job using the training script `train.py`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point='train.py',\n",
    "    train_instance_type=\"ml.m5.xlarge\",\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script `train.py` trains a logistic regression model on the training data, and saves the model to the `/opt/ml/model` directory, which Amazon SageMaker tars and uploads into a `model.tar.gz` file into S3 at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    training_data_directory = '/opt/ml/input/data/train'\n",
    "    train_features_data = os.path.join(training_data_directory, 'train_features.csv')\n",
    "    train_labels_data = os.path.join(training_data_directory, 'train_labels.csv')\n",
    "    print('Reading input data')\n",
    "    X_train = pd.read_csv(train_features_data, header=None)\n",
    "    y_train = pd.read_csv(train_labels_data, header=None)\n",
    "\n",
    "    model = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "    print('Training LR model')\n",
    "    model.fit(X_train, y_train)\n",
    "    model_output_directory = os.path.join('/opt/ml/model', \"model.joblib\")\n",
    "    print('Saving model to {}'.format(model_output_directory))\n",
    "    joblib.dump(model, model_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training job using `train.py` on the preprocessed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 14:30:56 Starting - Starting the training job...\n",
      "2020-06-02 14:30:58 Starting - Launching requested ML instances......\n",
      "2020-06-02 14:32:14 Starting - Preparing the instances for training......\n",
      "2020-06-02 14:33:21 Downloading - Downloading input data\n",
      "2020-06-02 14:33:21 Training - Downloading the training image...\n",
      "2020-06-02 14:33:52 Uploading - Uploading generated training model\u001b[34m2020-06-02 14:33:42,772 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:42,774 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:42,783 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:43,042 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:43,043 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:43,043 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:43,043 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=4774 sha256=c1b03f2f95c3dc6648101b246e0e33b4cbebfd2954fd7f9a5c835fdc685cea62\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rtovk_gy/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:44,361 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:44,371 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2020-06-02-14-30-55-584\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-30-55-584/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-30-55-584/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2020-06-02-14-30-55-584\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-30-55-584/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m train\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mReading input data\u001b[0m\n",
      "\u001b[34mTraining LR model\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\u001b[0m\n",
      "\u001b[34mSaving model to /opt/ml/model/model.joblib\u001b[0m\n",
      "\u001b[34m2020-06-02 14:33:47,339 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-06-02 14:33:59 Completed - Training job completed\n",
      "Training seconds: 57\n",
      "Billable seconds: 57\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({'train': preprocessed_training_data})\n",
    "training_job_description = sklearn.jobs[-1].describe()\n",
    "model_data_s3_uri = '{}{}/{}'.format(\n",
    "    training_job_description['OutputDataConfig']['S3OutputPath'],\n",
    "    training_job_description['TrainingJobName'],\n",
    "    'output/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-30-55-584/output/model.tar.gz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 14:33:55       1349 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {model_data_s3_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "`evaluation.py` is the model evaluation script. Since the script also runs using scikit-learn as a dependency,  run this using the `SKLearnProcessor` you created previously. This script takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    model_path = os.path.join('/opt/ml/processing/model', 'model.tar.gz')\n",
    "    print('Extracting model from path: {}'.format(model_path))\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    print('Loading model')\n",
    "    model = joblib.load('model.joblib')\n",
    "\n",
    "    print('Loading test input data')\n",
    "    test_features_data = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_labels_data = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "\n",
    "    X_test = pd.read_csv(test_features_data, header=None)\n",
    "    y_test = pd.read_csv(test_labels_data, header=None)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print('Creating classification evaluation report')\n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True)\n",
    "    report_dict['accuracy'] = accuracy_score(y_test, predictions)\n",
    "    report_dict['roc_auc'] = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print('Classification report:\\n{}'.format(report_dict))\n",
    "\n",
    "    evaluation_output_path = os.path.join('/opt/ml/processing/evaluation', 'evaluation.json')\n",
    "    print('Saving classification report to {}'.format(evaluation_output_path))\n",
    "\n",
    "    with open(evaluation_output_path, 'w') as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-06-02-14-34-38-906\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-30-55-584/output/model.tar.gz', 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/processing/output/test', 'LocalPath': '/opt/ml/processing/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-34-38-906/input/code/evaluation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'evaluation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-scikit-learn-2020-06-02-14-34-38-906/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mExtracting model from path: /opt/ml/processing/model/model.tar.gz\u001b[0m\n",
      "\u001b[34mLoading model\u001b[0m\n",
      "\u001b[34mLoading test input data\u001b[0m\n",
      "\u001b[34mCreating classification evaluation report\u001b[0m\n",
      "\u001b[34mClassification report:\u001b[0m\n",
      "\u001b[34m{'0': {'precision': 0.9413881748071979, 'recall': 0.7530552291421857, 'f1-score': 0.8367553451934063, 'support': 17020}, '1': {'precision': 0.38829864648522777, 'recall': 0.7697634160415464, 'f1-score': 0.5162039276385798, 'support': 3466}, 'micro avg': {'precision': 0.7558820658010349, 'recall': 0.7558820658010349, 'f1-score': 0.7558820658010349, 'support': 20486}, 'macro avg': {'precision': 0.6648434106462129, 'recall': 0.7614093225918661, 'f1-score': 0.6764796364159931, 'support': 20486}, 'weighted avg': {'precision': 0.8478116686486532, 'recall': 0.7558820658010349, 'f1-score': 0.7825216630082541, 'support': 20486}, 'accuracy': 0.7558820658010349, 'roc_auc': 0.7614093225918661}\u001b[0m\n",
      "\u001b[34mSaving classification report to /opt/ml/processing/evaluation/evaluation.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sklearn_processor.run(code='evaluation.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                                  source=model_data_s3_uri,\n",
    "                                  destination='/opt/ml/processing/model'),\n",
    "                              ProcessingInput(\n",
    "                                  source=preprocessed_test_data,\n",
    "                                  destination='/opt/ml/processing/test')],\n",
    "                      outputs=[ProcessingOutput(output_name='evaluation',\n",
    "                                  source='/opt/ml/processing/evaluation')]\n",
    "                     )                    \n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now retrieve the file `evaluation.json` from Amazon S3, which contains the evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": {\n",
      "        \"f1-score\": 0.8367553451934063,\n",
      "        \"precision\": 0.9413881748071979,\n",
      "        \"recall\": 0.7530552291421857,\n",
      "        \"support\": 17020\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"f1-score\": 0.5162039276385798,\n",
      "        \"precision\": 0.38829864648522777,\n",
      "        \"recall\": 0.7697634160415464,\n",
      "        \"support\": 3466\n",
      "    },\n",
      "    \"accuracy\": 0.7558820658010349,\n",
      "    \"macro avg\": {\n",
      "        \"f1-score\": 0.6764796364159931,\n",
      "        \"precision\": 0.6648434106462129,\n",
      "        \"recall\": 0.7614093225918661,\n",
      "        \"support\": 20486\n",
      "    },\n",
      "    \"micro avg\": {\n",
      "        \"f1-score\": 0.7558820658010349,\n",
      "        \"precision\": 0.7558820658010349,\n",
      "        \"recall\": 0.7558820658010349,\n",
      "        \"support\": 20486\n",
      "    },\n",
      "    \"roc_auc\": 0.7614093225918661,\n",
      "    \"weighted avg\": {\n",
      "        \"f1-score\": 0.7825216630082541,\n",
      "        \"precision\": 0.8478116686486532,\n",
      "        \"recall\": 0.7558820658010349,\n",
      "        \"support\": 20486\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluation_output_config = evaluation_job_description['ProcessingOutputConfig']\n",
    "for output in evaluation_output_config['Outputs']:\n",
    "    if output['OutputName'] == 'evaluation':\n",
    "        evaluation_s3_uri = output['S3Output']['S3Uri'] + '/evaluation.json'\n",
    "        break\n",
    "\n",
    "evaluation_output = S3Downloader.read_file(evaluation_s3_uri)\n",
    "evaluation_output_dict = json.loads(evaluation_output)\n",
    "print(json.dumps(evaluation_output_dict, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running processing jobs with your own dependencies\n",
    "\n",
    "Above, you used a processing container that has scikit-learn installed, but you can run your own processing container in your processing job as well, and still provide a script to run within your processing container.\n",
    "\n",
    "Below, you walk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container. Create a scikit-learn container and run a processing job using the same `preprocessing.py` script you used above. You can provide your own dependencies inside this container to run your processing script with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘docker’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Dockerfile to create the processing container. Install `pandas` and `scikit-learn` into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install pandas==0.25.3 scikit-learn==0.21.3\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code builds the container using the `docker` command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/4 : FROM python:3.7-slim-buster\n",
      " ---> 87b1022604d5\n",
      "Step 2/4 : RUN pip3 install pandas==0.25.3 scikit-learn==0.21.3\n",
      " ---> Using cache\n",
      " ---> 2c7b8aff2198\n",
      "Step 3/4 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 6914ac5b2b00\n",
      "Step 4/4 : ENTRYPOINT [\"python3\"]\n",
      " ---> Using cache\n",
      " ---> aa561fe012a6\n",
      "Successfully built aa561fe012a6\n",
      "Successfully tagged sagemaker-processing-container:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'sagemaker-processing-container' already exists in the registry with id '057716757052'\n",
      "The push refers to repository [057716757052.dkr.ecr.us-east-1.amazonaws.com/sagemaker-processing-container]\n",
      "\n",
      "\u001b[1B82057f08: Preparing \n",
      "\u001b[1B068e8a69: Preparing \n",
      "\u001b[1B464db597: Preparing \n",
      "\u001b[1B0ce58669: Preparing \n",
      "\u001b[1B6e8168dc: Preparing \n",
      "\u001b[1Bb21953f4: Layer already exists \u001b[1A\u001b[1K\u001b[Klatest: digest: sha256:877f37655076be7a942128ae9ac24088884518e472c0c274b23b3a4f39aa8076 size: 1583\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'sagemaker-processing-container'\n",
    "tag = ':latest'\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside this container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=processing_repository_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same `preprocessing.py` script you ran above, but now, this code is running inside of the Docker container you built in this notebook, not the scikit-learn image maintained by Amazon SageMaker. You can add the dependencies to the Docker image, and run your own pre-processing, feature-engineering, and model evaluation scripts inside of this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-processing-container-2020-06-02-14-38-53-364\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-processing-container-2020-06-02-14-38-53-364/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-processing-container-2020-06-02-14-38-53-364/output/train_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-processing-container-2020-06-02-14-38-53-364/output/test_data', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\n",
      "\u001b[34mReceived arguments Namespace(train_test_split_ratio=0.2)\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/census-income.csv\u001b[0m\n",
      "\u001b[34mData after cleaning: (68285, 9), 11401 positive examples, 56884 negative examples\u001b[0m\n",
      "\u001b[34mSplitting data into train and test sets with ratio 0.2\u001b[0m\n",
      "\u001b[34mRunning preprocessing and feature engineering transformations\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:778: DeprecationWarning: `make_column_transformer` now expects (transformer, columns) as input tuples instead of (columns, transformer). This has been introduced in v0.20.1. `make_column_transformer` will stop accepting the deprecated (columns, transformer) order in v0.22.\n",
      "  warnings.warn(message, DeprecationWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_discretization.py:193: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\u001b[0m\n",
      "\u001b[34mTrain data shape after preprocessing: (54628, 69)\u001b[0m\n",
      "\u001b[34mTest data shape after preprocessing: (13657, 69)\u001b[0m\n",
      "\u001b[34mSaving training features to /opt/ml/processing/train/train_features.csv\u001b[0m\n",
      "\u001b[34mSaving test features to /opt/ml/processing/test/test_features.csv\u001b[0m\n",
      "\u001b[34mSaving training labels to /opt/ml/processing/train/train_labels.csv\u001b[0m\n",
      "\u001b[34mSaving test labels to /opt/ml/processing/test/test_labels.csv\u001b[0m\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-sample-data-us-east-1/processing/census/census-income.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-processing-container-2020-06-02-14-38-53-364/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-processing-container-2020-06-02-14-38-53-364/output/train_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/sagemaker-processing-container-2020-06-02-14-38-53-364/output/test_data', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-processing-container-2020-06-02-14-38-53-364', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '057716757052.dkr.ecr.us-east-1.amazonaws.com/sagemaker-processing-container:latest', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocessing.py'], 'ContainerArguments': ['--train-test-split-ratio', '0.2']}, 'RoleArn': 'arn:aws:iam::057716757052:role/service-role/AmazonSageMaker-ExecutionRole-20191128T110038', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:057716757052:processing-job/sagemaker-processing-container-2020-06-02-14-38-53-364', 'ProcessingJobStatus': 'Completed', 'ProcessingEndTime': datetime.datetime(2020, 6, 2, 14, 42, 53, tzinfo=tzlocal()), 'ProcessingStartTime': datetime.datetime(2020, 6, 2, 14, 42, 31, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2020, 6, 2, 14, 42, 53, 828000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 6, 2, 14, 38, 53, 772000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'f2cd72ad-3b09-4d91-ad62-bed7c923e8dd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f2cd72ad-3b09-4d91-ad62-bed7c923e8dd', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2007', 'date': 'Tue, 02 Jun 2020 14:43:04 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "script_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "print(script_processor_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
